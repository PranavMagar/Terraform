''' PRACTICAL - 4 '''

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def simple_crawler(start_url, max_depth=2, visited_urls=None, current_depth=0):
    if visited_urls is None:
        visited_urls = set()
    if current_depth > max_depth or start_url in visited_urls:
        return

    print(f"Crawling: {start_url} (Depth: {current_depth})")
    visited_urls.add(start_url)

    try:
        response = requests.get(start_url)
        response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {start_url}: {e}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')

    for link in soup.find_all('a', href=True):
        next_url = urljoin(start_url, link['href'])
        # Optionally, add filtering to stay within a specific domain
        simple_crawler(next_url, max_depth, visited_urls, current_depth + 1)

if __name__ == "__main__":
    initial_url = "https://books.toscrape.com/"  # Example target URL
    simple_crawler(initial_url, max_depth=1)  # Crawl up to depth 1
